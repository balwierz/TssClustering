//============================================================================
// Name        : clustering.cpp
// Author      : 
// Version     :
// Copyright   : Your copyright notice
// Description : it shoule be run for each chr-strand combination separately
//					for speedups
//============================================================================
#include <fstream>
#include <iostream>
#include <deque>
#include <boost/unordered_map.hpp>
#include <boost/unordered_set.hpp>
#include <boost/foreach.hpp>
#include <boost/filesystem.hpp>
#include <boost/filesystem/operations.hpp>
#include <boost/filesystem/path.hpp>
#include <boost/array.hpp>
using namespace std;
namespace fs = boost::filesystem;
typedef boost::unordered_set<std::string> setT;

struct expression
{
	double raw;  // could be int, but then we have a conversion each time...
	double norm;
	expression(double r, double n) : raw(r), norm(n) {}
	expression(const expression& org) // maybe remove for speed ups
	{
		raw = org.raw;
		norm = org.norm;
	}
	expression() {}
};
typedef boost::unordered_map<std::string, expression> exprHashT; //key is a sample
struct TSC
{
	int	repr;
	int beg;
	int end;
	exprHashT expr; // hey is a sample
	TSC() {}
};
typedef boost::unordered_map<int, TSC> tscHashT; // key is a TSC ID. // in principle could be int

string exprDirSplit ("/import/bc2/home/nimwegen/GROUP/Promoterome/ResultsHuman/NormalizedSplit/");
string mappedDir ("/import/bc2/home/nimwegen/GROUP/Promoterome/ResultsHuman/Mapped/");

const int maxSamples = 1024;

int cmpInt (const void * a, const void * b)
{
  return ( *(int*)a - *(int*)b );
}
int nSamples;
boost::array<string,maxSamples> samples;
boost::array<double,maxSamples> total;

/********** CLUSTERING PARAMETERS ***********/
double sigmasq;  // noise size
double alpha;    // gaussian prior size for TSS variance
double lenscale; // coexpression prior scale depending on the distance


int main(int argc, char* argv[]) // argv[1] - which chrStr to do
{
	// determine the number of samples
	nSamples = 0;
	fs::path normalizedPath( exprDirSplit + argv[1] + "/");
	fs::directory_iterator end_iter;
	tscHashT allTSCs;
	for ( fs::directory_iterator dir_itr( normalizedPath ); dir_itr != end_iter; ++dir_itr )
	{
		if ( fs::is_regular_file( dir_itr->status() ) )
		{
			cout << dir_itr -> path().filename() << endl;
			deque<expression> TSC;
			//string suffix (".nr");
			string file = dir_itr->path().filename();
			//file.replace(file.find(suffix), suffix.length(), "");
			ifstream r ( (exprDirSplit + file).c_str(), ifstream::in );
			r >> skipws;
			while(r.good())
			{
				int pos;
				double norm;
				double raw;
				char str;
				string chrom;
				r >> chrom >> str >> pos >> raw >> norm;
				allTSCs[pos].expr[file].norm = norm;
				allTSCs[pos].expr[file].raw	 = raw;
				allTSCs[pos].beg = pos;
				allTSCs[pos].end = pos;
				allTSCs[pos].repr = pos;
			}
			r.close();
			r.open( (mappedDir + file).c_str() );
			r >> total[nSamples];
			r.close();
			nSamples ++;
		}
	}
	cout << nSamples << " samples read" << endl;
	// build a sorted deque of the TSCs

	int nTSS = allTSCs.size();
	int *allPositions = new int[nTSS];
	int i=0;
	BOOST_FOREACH(tscHashT::value_type elem, allTSCs)
	{
		allPositions[i++] = elem.first; // that is the position, which is a key.
	}
	qsort(allPositions, nTSS, sizeof(int), cmpInt);
	deque<TSC*> workTSCs;
	for(i = 0; i < nTSS; ++i )
	{
		workTSCs.push_back(&allTSCs[allPositions[i]]); // I hope we are pushing an address of a real object, and not a copy
	}
	// do the clustering

	delete allPositions;
	return 0;
}

double altprofprob(TSC* segone, TSC* segtwo)
// we might remove the second one in case we only want to cluster i and i+1...
{
	//double meandel = 0;
	//double vardel = 0;
	//int numexpress = 0;
	//int numone = 0;
	double Lindep, L, avwzz, avwz, avw;
	double avgamma, avgammarho, avgammarhorho, avgammasig, avgammasigsig, avgammarhosig;
	double betax, betay, avbetax, avbetay, avbetax_xx, avbetay_yy, avbetax_x, avbetay_y;
	//double pos, posx, posy;
	L = 0;
	Lindep = 0;

	avwzz = 0;
	avwz = 0;
	avgamma = 0;
	avw = 0;
	avgammarho = 0;
	avgammarhorho = 0;
	avgammasigsig = 0;
	avgammasig = 0;
	avgammarhosig = 0;

	betax = 0;
	betay = 0;
	avbetax = 0;
	avbetay = 0;
	avbetax_xx = 0;
	avbetay_yy = 0;
	avbetax_x = 0;
	avbetay_y = 0;

	// go over all the samples in which we have expression of the first TSC
	// and check if we have also expression in the second
	// if true, then do the calculations.
	int pos = 0; // keeps the number of used samples
	BOOST_FOREACH(exprHashT::value_type pair, segone->expr)
	{
		string sample = pair.first;
		if(segtwo->expr.find(pair.first) != segtwo->expr.end())
		{
			double n = segone->expr[sample].raw;
			double m = segtwo->expr[sample].raw;
			double nnor = segone->expr[sample].norm;
			double mnor = segtwo->expr[sample].norm;
			// here can be some pseudocounts
			pos++;
			double x = log(nnor);
			double y = log(mnor);
			double z = x - y;
			double s = 0.5 * (x + y);
			double wx = 1 / (sigmasq + 1.0 / n);
			double wy = 1 / (sigmasq + 1.0 / m);
			double w = wx * wy * (1.0 + alpha / (wx + wy)) / (wx + wy + alpha);
			double rho = 0.5 * (wx - wy) / (wx + wy);
			double sig = s + rho * z;
			double gamma = alpha * (wx + wy) / (wx + wy + alpha);

			avwzz += w * z * z;
			avwz += w * z;
			avgammasigsig += gamma * sig * sig;
			avgammasig += gamma * sig;
			avgamma += gamma;
			avw += w;
			avgammarho += gamma * rho;
			avgammarhorho += gamma * rho * rho;
			avgammarhosig += gamma * rho * sig;

			L += 0.5 * log(wx * wy * alpha / (wx + wy + alpha));

			double betax = alpha * wx / (alpha + wx);
			double betay = alpha * wy / (alpha + wy);
			avbetax += betax;
			avbetay += betay;
			avbetax_xx += betax * x * x;
			avbetay_yy += betay * y * y;
			avbetax_x += betax * x;
			avbetay_y += betay * y;

			Lindep += 0.5 * log(betax * betay);
		}
	}
	/**normalize all the averages****/
	if(pos > 0)
	{
		avwzz /= ((double) pos);
		avw /= ((double) pos);
		avwz /= ((double) pos);
		avgamma /= ((double) pos);
		avgammasig /= ((double) pos);
		avgammasigsig /= ((double) pos);
		avgammarho /= ((double) pos);
		avgammarhorho /= ((double) pos);
		avgammarhosig /= ((double) pos);

		avbetax /= ((double) pos);
		avbetay /= ((double) pos);
		avbetax_x /= ((double) pos);
		avbetay_y /= ((double) pos);
		avbetax_xx /= ((double) pos);
		avbetay_yy /= ((double) pos);

		/***contribution from the prefactor***/
		L -= 0.5 * log( avgamma * avw + avgamma * avgammarhorho - avgammarho * avgammarho);
		/**Note the 1/Nsamp, the 2pi's,  and the 1/(mumax-mumin) will cancel between dependent and independent model***/

		/***Expression in the exponent of the dependent model***/
		double Q = avwzz + avgammasigsig - avgammasig * avgammasig / avgamma;
		Q -= (avwz + avgammarhosig - avgammarho * avgammasig / avgamma) * (avwz + avgammarhosig - avgammarho * avgammasig / avgamma) / (avw + avgammarhorho - avgammarho * avgammarho / avgamma);

		L -= 0.5 * ((double) pos) *  Q;

		/***Now contribution from independent model***/
		Lindep -= 0.5 * ((double) pos) * (avbetax_xx - (avbetax_x * avbetax_x / avbetax));
		Lindep -= 0.5 * ((double) pos) * (avbetay_yy - (avbetay_y * avbetay_y / avbetay));
		Lindep -= 0.5 * log( avbetax * avbetay);

		/***Difference between the log-likelihoods***/
		L -= Lindep;
	}
	else
	{
		L = 0;
	}
	return L;
}
 /*

	for(i = 0; i < nSamples; ++i) // over samples
	{
		string sample = samples[i];
		n = segone->expr[sample].norm;
		m = segtwo->expr[sample].norm;

		if(n >= 1 || m >= 1 )
		{
			++pos;

			nnor = segone->normprofile[i];
			mnor = segtwo->normprofile[i];
			if(n < 1)
			{
				n = 1;
				nnor += 0.5 * 1000000.0 / (tottag[i] + 1.0);
			}
			else
			{
				++posx;
			}

			if(m < 1)
			{
				m = 1;
				mnor += 0.5 * 1000000.0 / (tottag[i] + 1.0);
			}
			else
			{
				++posy;
			}

			x = log(nnor);
			y = log(mnor);
			z = x - y;
			s = 0.5 * (x + y);
			wx = 1 / (sigmasq[i] + 1.0 / n);
			wy = 1 / (sigmasq[i] + 1.0 / m);
			w = wx * wy * (1.0 + alpha / (wx + wy)) / (wx + wy + alpha);
			rho = 0.5 * (wx - wy) / (wx + wy);
			sig = s + rho * z;
			gamma = alpha * (wx + wy) / (wx + wy + alpha);

			avwzz += w * z * z;
			avwz += w * z;
			avgammasigsig += gamma * sig * sig;
			avgammasig += gamma * sig;
			avgamma += gamma;
			avw += w;
			avgammarho += gamma * rho;
			avgammarhorho += gamma * rho * rho;
			avgammarhosig += gamma * rho * sig;

			L += 0.5 * log( wx * wy * alpha / (wx + wy + alpha));

			betax =  alpha * wx / (alpha + wx);
			betay =  alpha * wy / (alpha + wy);
			avbetax += betax;
			avbetay += betay;
			avbetax_xx += betax * x * x;
			avbetay_yy += betay * y * y;
			avbetax_x += betax * x;
			avbetay_y += betay * y;

			Lindep += 0.5 * log(betax * betay);
		}
	}
	//normalize all the averages
	if(pos > 0)
	{
		avwzz /= ((double) pos);
		avw /= ((double) pos);
		avwz /= ((double) pos);
		avgamma /= ((double) pos);
		avgammasig /= ((double) pos);
		avgammasigsig /= ((double) pos);
		avgammarho /= ((double) pos);
		avgammarhorho /= ((double) pos);
		avgammarhosig /= ((double) pos);

		avbetax /= ((double) pos);
		avbetay /= ((double) pos);
		avbetax_x /= ((double) pos);
		avbetay_y /= ((double) pos);
		avbetax_xx /= ((double) pos);
		avbetay_yy /= ((double) pos);

		//contribution from the prefactor
		L -= 0.5 * log( avgamma * avw + avgamma * avgammarhorho - avgammarho * avgammarho);
		//Note the 1/Nsamp, the 2pi's,  and the 1/(mumax-mumin) will cancel between dependent and independent model

		//Expression in the exponent of the dependent model
		Q = avwzz + avgammasigsig - avgammasig * avgammasig / avgamma;
		Q -= (avwz + avgammarhosig - avgammarho * avgammasig / avgamma) * (avwz + avgammarhosig - avgammarho * avgammasig / avgamma) / (avw + avgammarhorho - avgammarho * avgammarho / avgamma);

		L -= 0.5 * ((double) pos) *  Q;

		//Now contribution from independent model
		Lindep -= 0.5 * ((double) pos) * (avbetax_xx - (avbetax_x * avbetax_x / avbetax));
		Lindep -= 0.5 * ((double) pos) * (avbetay_yy - (avbetay_y * avbetay_y / avbetay));
		Lindep -= 0.5 * log( avbetax * avbetay);

		//Difference between the log-likelihoods
		L -= Lindep;
	}
	else
	{
		L = 0;
	}
	return L;
}
*/

/**prior probability for fusing segments depending on their distance***/
double priorprob(TSC *segone, TSC *segtwo)
{
	int dist = segtwo->beg - segone->end + 1;
	/***test use instead distances between representative positions***/
	/*dist = segtwo->reppos-segone->reppos+1;*/
	double prior = -((double) dist) / lenscale;
	prior -= log(1.0 - exp(prior));
	return prior;
}
